#!/usr/bin/env python
# coding: utf-8

# # NASA Turbofan Jet Engine Data Set

# The Data Incubator Capstone Project 092021<br>
# Islam Abdelmotalib<br>
# Last Update: 12/05/2021

# # Data Source

# NASA Turbofan Jet Engine Data Set
# This Data set generated by NASA team using C-MAPSS simulator, The software is coded in the MATLAB® and Simulink® environment. for more data about how this data simulated check the referenece 
# 
# Reference: A. Saxena, K. Goebel, D. Simon, and N. Eklund, Damage Propagation Modeling for Aircraft Engine Run-to-Failure Simulation, in the Proceedings of the 1st International Conference on Prognostics and Health Management (PHM08), Denver CO, Oct 2008.
# 
# The Data Source is Kaggle 
# https://www.kaggle.com/behrad3d/nasa-cmaps

# # Data Description

# This Data Set consist of four different data set simulated under different combinations of operational conditions and fault modes. Each data set divided into train and test, Train tables include time stamps (cycles) for each engine till failure, Test table include number of cycles for each engine before failure
# 
# The columns correspond to:
# 1) unit number
# 
# 2) time, in cycles
# 
# 3) operational setting 1
# 
# 4) operational setting 2
# 
# 5) operational setting 3
# 
# 6) sensor measurement 1
# 
# 7) sensor measurement 2
# 
# …
# 
# 26) sensor measurement 26

# # Project Objective

# The objective to estimate the remaining useful life (RUL) for each engine in the Test Data Set. Reliably estimating remaining life could have many advantages as:
# 
# 1) Cost savings (for example by avoiding unscheduled maintenance and by increasing equipment usage)
# 
# 2) Operational safety improvements
# 
# 3) Provide decision makers with information that allows them to change operational characteristics (such as load) which in turn may prolong the life of the component. 
# 
# 4) It also allows planners to account for upcoming maintenance and set in motion a logistics process that supports a smooth transition from faulty equipment to fully functional. 

# # Data Ingestion

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline


# In[2]:


FD001_train = pd.read_csv('Dataset/train_FD001.txt', sep=" ", header=None)
FD001_test = pd.read_csv('Dataset/test_FD001.txt', sep=" ", header=None)
FD001_RUL0 = pd.read_csv('Dataset/RUL_FD001.txt', sep=" ", header=None)
#FD001_train.columns = ["a", "b", "c", "etc."]


# In[3]:


Columns=['unit', 'cycle','op1','op2','op3']+[f'sensor{x}' for x in range(1,24)]
Columns


# In[4]:


FD001_train.columns = Columns
FD001_test.columns = Columns


# In[5]:


FD001_train


# In[6]:


FD001_train.info()


# In[7]:


FD001_train.describe()


# In[8]:


FD001_train['op3'].unique()


# In[9]:


plt.plot(np.sort(FD001_train['op1'].unique()))


# In[10]:


plt.plot(np.sort(FD001_train['op2'].unique()))


# In[11]:


pd.set_option('display.max_columns', None)


# In[12]:


# Getting the RUL for each line
RUL_max=FD001_train.groupby('unit').max()['cycle'].to_frame().rename(columns={"cycle": "Max_RUL"})
FD001_RUL=FD001_train.merge(RUL_max,how='left', on='unit')
FD001_RUL['RUL']=FD001_RUL['Max_RUL']-FD001_RUL['cycle']
FD001_RUL


# In[13]:


for i in FD001_RUL.columns[5:28]:
    for j in range(1,100,10):
        plt.plot(FD001_RUL[FD001_RUL['unit']==j]['RUL'][::-1],FD001_RUL[FD001_RUL['unit']==j][i])
    plt.xlabel('remaining useful life')
    plt.ylabel(i)
    plt.show()


# the Graphs above shows that these factors
# 
# ['op3','sensor1','sensor5','sensor10','sensor16','sensor18','sensor19','sensor22','sensor23']
# 
# don't have any variations againist the RUL, so will drop it from the training set

# In[14]:


FD001_RUL=FD001_RUL.drop(labels=['op3','sensor1','sensor5','sensor10','sensor16','sensor18','sensor19','sensor22','sensor23'],axis=1)


# The Charts above it shows that after 150 cycles the charts start to be random and it loses the patterns like senor 9 and 14
# 
# so I decided to drop all data after 150 cycles
# 

# In[15]:


FD001_RUL_150=FD001_RUL[FD001_RUL['RUL']<150]
FD001_RUL_150


# # Machine Learning:

# In[16]:


Final_Model0=Pipeline([
    ('Scaler',MinMaxScaler()),
    ('Ply_Feature', PolynomialFeatures(2)),
    ('Regressor',Ridge(alpha=1)) 
])


# 

# In[17]:


Final_Model0.fit(FD001_RUL_150[FD001_RUL_150.columns[4:19]],FD001_RUL_150['RUL'])


# In[29]:


FD001_test


# In[30]:


FD001_test=FD001_test.drop(labels=['op3','sensor1','sensor5','sensor10','sensor16','sensor18','sensor19','sensor22','sensor23'],axis=1)
FD001_test


# In[31]:


x_test=FD001_test.groupby('unit').last('cycle').drop(labels=['cycle','op1','op2'],axis=1)


# In[32]:


x_test


# In[22]:


y_predict=Final_Model0.predict(x_test)


# In[23]:


y_predict


# In[24]:


FD001_RUL0[0]


# In[25]:


plt.plot(FD001_RUL0[0])
plt.plot(y_predict)


# In[26]:


Final_Model0.score(x_test,FD001_RUL0[0])


# In[16]:


# Random Forest Model
from sklearn.ensemble import RandomForestRegressor


# In[17]:


Final_Model1=Pipeline([
    ('Scaler',MinMaxScaler()),
    ('Ply_Feature', PolynomialFeatures(2)),
    ('Regressor',RandomForestRegressor(max_depth=3, random_state=0)) 
])


# In[18]:


Final_Model1.fit(FD001_RUL_150[FD001_RUL_150.columns[4:19]],FD001_RUL_150['RUL'])


# In[33]:


y_predict1=Final_Model1.predict(x_test)


# In[34]:


Final_Model1.score(x_test,FD001_RUL0[0])


# In[ ]:





# # Visualizations:

# In[27]:


pd.DataFrame(data=FD001_test.groupby('unit').last('cycle')['cycle'].reset_index())


# In[28]:


viz01=pd.DataFrame(data=y_predict,columns=['Predicted RUL'])
viz01['Actual RUL']=FD001_RUL0[0]
viz01['current cycle']=pd.DataFrame(data=FD001_test.groupby('unit').last('cycle')['cycle'].reset_index())['cycle']
viz01['Predicted Life']=viz01['Predicted RUL']+viz01['current cycle']
viz01['Actual Life']=viz01['Actual RUL']+viz01['current cycle']
viz01['ave Life']=np.average(RUL_max['Max_RUL'])
viz01['Diff from avg']=viz01['Predicted Life']-viz01['ave Life']
viz01['Error']=((viz01['Actual Life']-viz01['Predicted Life'])/viz01['Actual Life']).abs()
viz01


# In[29]:


sns.barplot(x=x_test.reset_index()['unit'], y=viz01['Diff from avg'])


# In[30]:


negative=np.average(viz01[viz01['Diff from avg']<=0]['Diff from avg'])
positive=np.average(viz01[viz01['Diff from avg']>0]['Diff from avg'])

polarity=['Safety concern' if x <= 0 else 'Cost Concern' for x in viz01['Diff from avg']]
polarity_avg=[negative if x <= 0 else positive for x in viz01['Diff from avg']]


# In[42]:


viz01[viz01['Diff from avg']<=0]['Diff from avg'].shape


# In[31]:


viz01['Concern']=polarity
viz01['avg_concern']=polarity_avg
viz01['unit']=x_test.reset_index()['unit']
viz01


# In[32]:


import seaborn as sns
sns.set_theme(style="ticks")


# Show the results of a linear regression within each dataset
ax01=sns.lmplot(x="unit", y="Diff from avg", col="Concern", hue="Concern", data=viz01,
           col_wrap=2, ci=None, palette="muted", height=4,
           scatter_kws={"s": 50, "alpha": 1})


# In[ ]:





# In[33]:


ax02 = sns.boxplot( y="Error", data=viz01).set(title='Actual Life vs Predicted Life Precentage Error',
                                            ylabel="Error Percentage",
                                            xlabel="Turbo Fan Jet Engine Set")


# In[ ]:





# Violin Chart

# In[34]:


Life=FD001_test.groupby('unit').last('cycle')['cycle'].to_frame()
Life['RUL']=y_predict
Life['Jet_Engine_Life']=Life['RUL']+Life['cycle']
Life['DataSet']='Test DataSet'


# In[35]:


Life
life_test=Life[['Jet_Engine_Life','DataSet']].reset_index()
life_test


# In[36]:


RUL_max=FD001_train.groupby('unit').max()['cycle'].to_frame().rename(columns={"cycle": "Jet_Engine_Life"}).reset_index()
RUL_max['DataSet']='Training DataSet'
Draw0=RUL_max.append(life_test).reset_index()


# In[37]:


Draw0


# In[38]:



sns.set_theme(style="whitegrid")


# Draw a nested violinplot and split the violins for easier comparison
sns.violinplot(data=Draw0, y="Jet_Engine_Life", x="DataSet",
               split=False, inner="quart", linewidth=1)
sns.despine(left=True)


# In[ ]:




